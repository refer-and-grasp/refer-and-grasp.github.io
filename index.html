<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Refer and Grasp: Vision-Language Guided Continuous Dexterous Grasping</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css">
    <style>
        body {
            font-family: "Times New Roman", serif;
            margin: 0;
            padding: 0;
            background-color: #ffffff;
            color: #333;
        }

        header {
            background-color: #f4f4f4;
            text-align: center;
            padding: 20px 0;
            /* border-bottom: 2px solid #ddd; */
        }

        header h1 {
            font-size: 3.2rem;
            margin-bottom: 20px;
        }

        header p {
            margin-top: 5px;
            font-size: 1.2em;
        }

        .button-container {
            text-align: center;
            /* margin-top: 20px; */
        }

        .button-container button {
            padding: 15px 15px;
            margin: 0 10px;
            font-size: 1.2em;
            cursor: pointer;
            border: none;
            background-color: #4c5c69;
            color: white;
            border-radius: 15px;
            display: inline-flex;
            align-items: center;
            gap: 8px;
        }

        button.disabled {
            background-color: #ccc;
            cursor: not-allowed;
            color: white;
        }

        .button-container button i {
            font-size: 1.1em;
        }

        .button-container button:hover {
            background-color: #394854;
        }

        .button-container a {
            text-decoration: none;
        }

        button.disabled:hover {
            background-color: #ccc;
        }

        .content {
            max-width: 900px;
            margin: 30px auto;
            padding: 20px;
            /* background-color: white; */
            border-radius: 10px;
            /* box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); */
        }

        h2 {
            font-size: 2.5rem;
            margin-bottom: 10px;
        }

        .abstract,
        .main-content {
            font-size: 1.3rem;
            text-align: justify;
            /* text-justify: inter-word; */
            hyphens: auto;
            -webkit-hyphens: auto;
            -ms-hyphens: auto;
            text-indent: 1rem;
            /* word-break: break-word; */
            line-height: 1.5;
        }

        .video-container {
            display: flex;
            justify-content: space-between;
            /* gap: 20px; */
            margin: 20px 0;
        }

        .citation pre {
            margin-top: 20px;
            background-color: #f7f7f7;
            padding: 15px;
            border-radius: 5px;
            font-family: "Consolas", "Times New Roman", serif;
            font-size: 1em;
            line-height: 1.3;
            overflow-x: auto;
        }

        .dropdown {
            position: relative;
            display: inline-block;
        }

        .dropdown-content {
            display: block;
            position: absolute;
            background-color: #fbfbfb;
            min-width: 12em;
            box-shadow: 0 5px 10px 0 rgba(0, 0, 0, 0.2);
            z-index: 1;
            border-radius: 15px;
            margin-top: 10px;
            left: 50%;
            transform: translateX(-50%) translateY(-15px);
            /* 添加Y轴位移 */
            opacity: 0;
            /* 默认透明 */
            visibility: hidden;
            /* 默认隐藏 */
            transition: all 0.2s ease;
            /* 添加过渡效果 */
        }

        .dropdown-content a {
            color: black;
            padding: 20px 10px;
            text-decoration: none;
            display: block;
            font-size: 1em;
        }

        .dropdown-content .disabled {
            cursor: not-allowed;
        }


        .show {
            opacity: 1;
            /* 显示时完全不透明 */
            visibility: visible;
            /* 显示时可见 */
            transform: translateX(-50%) translateY(0);
            /* 取消Y轴位移 */
        }


        .dropdown-content a:hover {
            background-color: #f1f1f1;
            border-radius: 0px;
        }
    </style>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            document.querySelectorAll('.dropdown button').forEach(button => {
                button.addEventListener('click', function (event) {
                    event.stopPropagation();
                    const dropdownContent = this.nextElementSibling;

                    document.querySelectorAll('.dropdown-content').forEach(content => {
                        if (content !== dropdownContent && content.classList.contains('show')) {
                            content.classList.remove('show');
                        }
                    });

                    dropdownContent.classList.toggle('show');
                });
            });

            window.addEventListener('click', function (event) {
                if (!event.target.matches('.dropdown button') &&
                    !event.target.matches('.dropdown button *')) {
                    document.querySelectorAll('.dropdown-content').forEach(content => {
                        if (content.classList.contains('show')) {
                            content.classList.remove('show');
                        }
                    });
                }
            });
        });
    </script>
</head>

<body>

    <header>
        <h1>Refer and Grasp: Vision-Language Guided<br>Continuous Dexterous Grasping</h1>
        <p style="font-size: 1.5rem;">IROS 2025</p>
        <p>Yayu Huang<sup>1,2,*</sup>, Dongxuan Fan<sup>1,2,*</sup>, Wen Qi<sup>1,2</sup>, Daheng Li<sup>1</sup>, Yifan
            Yang<sup>1,2</sup>, Yongkang Luo<sup>1</sup>, Jia Sun<sup>1</sup>, Qian Liu<sup>1</sup>, Peng
            Wang<sup>1,2,3</sup><br><br>
            <span style="font-size: 0.9em;">
                <sup>1</sup>Institute of Automation, Chinese Academy of Sciences, Beijing, China<br>
                <sup>2</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing,
                China<br>
                <sup>3</sup>Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and
                Innovation, Chinese Academy of Sciences, Hong Kong<br>
                <sup>*</sup>Equal Contribution
            </span>
        </p>
        <div style="height: 1em;"></div>
        <div class="button-container">
            <a href="https://refer-and-grasp.github.io/IROS_Refer_and_Grasp.pdf">
                <button><i class="fas fa-file-lines"></i>Paper</button>
            </a>

            <a href="https://ieeexplore.ieee.org/document/11247415">
                <button><i class="fas fa-book"></i>IEEE</button>
            </a>

            <!-- <a href="null">
                <button class="disabled" title="Available Soon"><i class="fas fa-film"></i>Video</button>
            </a> -->

            <a href="https://github.com/Soappyooo/RefGrasp">
                <button><i class="fa-solid fa-database"></i>Dataset</button>
            </a>

            <a href="https://github.com/Soappyooo/Polyformer_RGBD">
                <button><i class="fa-brands fa-github"></i>Model</button>
            </a>

            <!-- <div class="dropdown">
                <button>
                    <i class="fa-brands fa-github"></i>Models
                </button>
                <div class="dropdown-content">
                    <a href="https://github.com/Soappyooo/Polyformer_RGBD">Visual Grounding Model</a>
                    <a href="null" title="Available Soon" class="disabled">Grasping Model</a>
                </div>
            </div> -->
        </div>
        <div style="height: 1em;"></div>
    </header>



    <div class="content">
        <h2>Abstract</h2>
        <p class="abstract">Robotic grasping guided by natural language instructions faces challenges due to ambiguities
            in object descriptions and the need to interpret complex spatial context. Existing visual grounding methods
            often rely on datasets that fail to capture these complexities, particularly when object categories are
            vague or undefined. To address these challenges, we make three key contributions. First, we present an
            automated dataset generation engine for visual grounding in tabletop grasping, combining procedural scene
            synthesis with template-based referring expression generation, requiring no manual labeling. Second, we
            introduce the RefGrasp dataset, featuring diverse indoor environments and linguistically challenging
            expressions for robotic grasping tasks. Third, we propose a visually grounded dexterous grasping framework
            with continuous grasp generation, validated through extensive real-world robotic experiments. Our work
            offers a novel approach for language-guided robotic manipulation, providing both a challenging dataset and
            an effective grasping framework for real-world applications.</p>

        <div class="video-container">
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_01-00_02_06.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_01_51-00_01_56.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div style="height: 1em;"></div>

        <h2>Dataset Generation Pipeline</h2>
        <p class="main-content">The dataset generation engine consists of a <b>scene graph generation process</b>, a
            <b>referring expression generation method</b> and a <b>scene rendering pipeline</b>. For an empty scene, we
            first procedurally generate its scene graph by randomly sampling nodes (objects) and edges (relationships
            between objects). Subsequently, referring expressions are constructed based on the scene graph using a
            variety of language templates. Next, object locations are determined according to the scene graph, with
            random selection of surface types, materials for floors and walls, lighting conditions, etc. Finally, the
            RGB-D images and segmentation masks are rendered using Blender's Cycles rendering engine.
        </p>
        <img src="./images/scene_graph_pipeline.jpg" alt="Dataset Generation Pipeline"
            style="width: 100%; margin: 0px 0;">
        <video style="width: 100%; margin: 20px 0;" autoplay loop muted playsinline>
            <source src="./videos/dataset.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <div style="height: 1em;"></div>

        <h2>Dataset Samples</h2>
        <p class="main-content">Figure below shows annotated examples from RefGrasp dataset. In the expressions, object
            references are marked with <span style="color:#4591a6">blue</span>, attributes are marked with <span
                style="color:#c15a51">red</span>,
            locations are marked with <span style="color:#7f9c4a">green</span> and related
            objects are marked with <span style="color:#c49d16">yellow</span>.</p>
        <img src="./images/dataset_example_x8.jpg" alt="Dataset Samples" style="width: 100%; margin: 0px 0;">
        <div style="height: 1em;"></div>

        <h2>Visually Grounded Dexterous Grasping</h2>
        <p class="main-content">The system is designed as a two-stage pipeline, with each stage involving a separate
            network trained individually. <b>Stage I</b> focuses on perception and segmentation tasks. The input to
            the network includes an RGB image, a depth image, and natural language instructions, and the output is the
            segmentation mask of the target object. <b>Stage II</b> addresses the reasoning and decision-making
            aspects of grasping. First, the depth image, segmentation mask, and robot's current state are fed into the
            grasp generation network. The output is the grasp configuration, which includes a sequence of actions for
            the robot to perform the grasp.</p>
        <img src="./images/framework.jpg" alt="Framework" style="width: 70%; display: block; margin: 30px auto;">
        <div style="height: 1em;"></div>

        <h2>Experiment Demos</h2>
        <div class="video-container">
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_01_46-00_01_50.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_01_51-00_01_56.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="video-container">
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_01_56-00_02_00.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_01-00_02_06.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="video-container">
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_06-00_02_10.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_11-00_02_15.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="video-container">
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_15-00_02_20.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_20-00_02_25.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="video-container">
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_25-00_02_30.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_30-00_02_35.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="video-container">
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_35-00_02_40.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_40-00_02_44.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="video-container">
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_44-00_02_49.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video width="49%" autoplay loop muted playsinline>
                <source src="./videos/video 00_02_49-00_02_54.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div style="height: 1em;"></div>

        <h2>Citation</h2>
        <div class="citation">
            <!-- <pre>
@misc{citekey,
      title={Title}, 
      author={author},
      year={2025},
      eprint={xxx},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/xxx}, 
}</pre> -->
            <pre>
@INPROCEEDINGS{11247415,
    author={Huang, Yayu and Fan, Dongxuan and Qi, Wen and Li, Daheng and Yang, Yifan and Luo, Yongkang and Sun, Jia and Liu, Qian and Wang, Peng},
    booktitle={2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    title={Refer and Grasp: Vision-Language Guided Continuous Dexterous Grasping},
    year={2025},
    volume={},
    number={},
    pages={7576-7583},
    keywords={Visualization;Grounding;Focusing;Grasping;Manuals;Robustness;Indoor environment;Labeling;Robots;Engines},
    doi={10.1109/IROS60139.2025.11247415}}
</pre>
        </div>
    </div>

    <footer style="text-align: center; padding: 20px 0; background-color: #f4f4f4; margin-top: 40px;">
        <a href="https://github.com/refer-and-grasp/refer-and-grasp.github.io"
            style="font-size: 2em; color: #333; text-decoration: none;" target="_blank"
            title="GitHub repository of this webpage">
            <i class="fab fa-github"></i>
        </a>
    </footer>

</body>

</html>
